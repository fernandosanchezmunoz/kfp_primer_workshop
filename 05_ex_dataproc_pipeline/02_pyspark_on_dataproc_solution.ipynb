{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a pyspark job on Dataproc\n",
    "\n",
    "1. You should use the component ```load_spark_to_GCS``` to upload ```transform_run.py``` and ```sparkicson-0.1-dependencies.jar```\n",
    "2. You should use the standard components [create_cluster](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/create_cluster), [submit_pyspark_job](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/submit_pyspark_job) and [delete_cluster](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/delete_cluster). ```kfp.components.ComponentStore``` could help.\n",
    "3. Use a template name for the cluster\n",
    "4. Check ```kfp.dsl.ExitHandler```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "from kfp import Client as KfpClient\n",
    "\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the component ```load_spark_to_GCS ``` from the local repository\n",
    "\n",
    "Hint use `kfp.components.ComponentStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = comp.ComponentStore(\n",
    "  local_search_paths=['components'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files_to_GCS_op = component_store.load_component('load_spark_to_GCS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the GCP components from github\n",
    "\n",
    "Hint use `kfp.components.ComponentStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_component_store = comp.ComponentStore(\n",
    "    url_search_prefixes=['https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_create_cluster_op = remote_component_store.load_component('dataproc/create_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_submit_pyspark_job_op = remote_component_store.load_component('dataproc/submit_pyspark_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_delete_cluster_op = remote_component_store.load_component('dataproc/delete_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = '{0}/kfp_primer/pyspark'.format('') ### insert your backet name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline\n",
    "\n",
    "1. We are going to query the table `bigquery-samples:wikipedia_benchmark.Wiki10M`.\n",
    "2. To pass complex types as paramenter use ``json.dumps`.\n",
    "3. Objectes of the class `kff.dsl.PipelineParams` should be cast to `str`.\n",
    "\n",
    "Here the pipeline you have to built\n",
    "![Pipeline](img/pyspark.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name = 'Test',\n",
    "    description = 'Simple pipeline to exeperiment with KFP'\n",
    ")\n",
    "def end_to_end_pyspark(\n",
    "    cluster_project_id = 'kfp-primer-workshop', \n",
    "    cluster_region = '', ### insert your region\n",
    "    cluster_name = 'spark-{{workflow.uid}}',\n",
    "    gcs_pkgs_path = 'gs://{0}/output/{{workflow.uid}}/{{pod.name}}/pkgs'.format(BUCKET_NAME),\n",
    "    bq_project_id = 'bigquery-samples',\n",
    "    bq_dataset = 'wikipedia_benchmark',\n",
    "    bq_table = 'Wiki10M',\n",
    "    output_path = 'gs://{0}/output/{{workflow.uid}}/{{pod.name}}/test.csv'.format(BUCKET_NAME),\n",
    "    args='',\n",
    "    job='{}',\n",
    "    wait_interval='30'\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    delete_cluster_task = dataproc_delete_cluster_op(\n",
    "        cluster_project_id,\n",
    "        cluster_region,\n",
    "        cluster_name\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        \n",
    "    with dsl.ExitHandler(delete_cluster_task):\n",
    "        #create cluster\n",
    "        create_cluster_task = dataproc_create_cluster_op(\n",
    "            project_id=cluster_project_id,\n",
    "            region=cluster_region,\n",
    "            name=cluster_name).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "        create_cluster_task.set_display_name('create cluster')\n",
    "        \n",
    "        #upload file to GCS\n",
    "        upload_files_to_GCS_task = upload_files_to_GCS_op(\n",
    "            output_gcs_path=gcs_pkgs_path).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "        upload_files_to_GCS_task.set_display_name('upload files')\n",
    "        \n",
    "        #submit job to dataproc cluster\n",
    "        dataproc_submit_pyspark_job_task = dataproc_submit_pyspark_job_op(\n",
    "            project_id=cluster_project_id, \n",
    "            region=cluster_region, \n",
    "            cluster_name=create_cluster_task.outputs['cluster_name'], \n",
    "            main_python_file_uri=upload_files_to_GCS_task.outputs['transform_run_path'], \n",
    "            args=args, \n",
    "            pyspark_job=json.dumps({\n",
    "                'main_python_file_uri': str(upload_files_to_GCS_task.outputs['transform_run_path']),\n",
    "                'jar_file_uris': str(upload_files_to_GCS_task.outputs['jar_path']),\n",
    "                'args' : ['--tableProjectID', str(bq_project_id), \n",
    "                          '--dataset', str(bq_dataset), \n",
    "                          '--table', str(bq_table),\n",
    "                          '--output', str(output_path)]\n",
    "            }),  \n",
    "            job=job, \n",
    "            wait_interval=wait_interval).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        \n",
    "        dataproc_submit_pyspark_job_task.set_display_name('run pyspark job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile pipeline to check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(end_to_end_pyspark, end_to_end_pyspark.__name__ + '.pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pipeline to Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running outside of the cluster with Kubeflow, set `GOOGLE_APPLICATION_CREDENTIALS` for dealing with authorisation. The service account needs to have the role `IAP-secured Web App User`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '' # path to the json file of the service account used to log in: it need to have role IAP-secured Web App User\n",
    "# HOST = '' # url of the cluster e.g. https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline\n",
    "# CLIENT_ID = '' # The client ID used by Identity-Aware Proxy\n",
    "# NAMESPACE = '' # user namespace e.g. https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KfpClient(\n",
    "# we are running into the same Kubeflow so we do not need to do anything\n",
    "#     host=HOST,\n",
    "#     client_id=CLIENT_ID,\n",
    "#     namespace=NAMESPACE  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_pipeline(\n",
    "    pipeline_package_path=end_to_end_pyspark.__name__ + '.pipeline.zip', \n",
    "    pipeline_name='e2e_pyspark_run_04') #make the name unique with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline from the UI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
