{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a pyspark job on Dataproc\n",
    "\n",
    "1. You should use the component ```load_spark_to_GCS``` to upload ```transform_run.py``` and ```sparkicson-0.1-dependencies.jar```\n",
    "2. You should use the standard components [create_cluster](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/create_cluster), [submit_pyspark_job](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/submit_pyspark_job) and [delete_cluster](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/delete_cluster). ```kfp.components.ComponentStore``` could help.\n",
    "3. Use a template name for the cluster\n",
    "4. Check ```kfp.dsl.ExitHandler```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "from kfp import Client as KfpClient\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = comp.ComponentStore(\n",
    "  local_search_paths=['components'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files_to_GCS_op = component_store.load_component('load_spark_to_GCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_component_store = comp.ComponentStore(\n",
    "    url_search_prefixes=['https://raw.githubusercontent.com/kubeflow/pipelines/master/components/gcp/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_create_cluster_op = remote_component_store.load_component('dataproc/create_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_submit_pyspark_job_op = remote_component_store.load_component('dataproc/submit_pyspark_job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataproc_delete_cluster_op = remote_component_store.load_component('dataproc/delete_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : remove before push\n",
    "dataproc_create_cluster_op = comp.load_component_from_file(\n",
    "    '/Users/lfloretta/IdeaProjects/pipelines/components/gcp/dataproc/create_cluster/component.yaml')\n",
    "dataproc_submit_pyspark_job_op = comp.load_component_from_file(\n",
    "    '/Users/lfloretta/IdeaProjects/pipelines/components/gcp/dataproc/submit_pyspark_job/component.yaml')\n",
    "dataproc_delete_cluster_op = comp.load_component_from_file(\n",
    "    '/Users/lfloretta/IdeaProjects/pipelines/components/gcp/dataproc/delete_cluster/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'lf-ml-demo-eu-w1/kfp_primer/test/dataproc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name = 'Test',\n",
    "    description = 'Simple pipeline to exeperiment with KFP'\n",
    ")\n",
    "def end_to_end_pyspark(\n",
    "    cluster_project_id = 'lf-ml-demo', \n",
    "    cluster_region = 'europe-west1',\n",
    "    cluster_name = '{{workflow.uid}}',\n",
    "    gcs_pkgs_path = 'gs://{0}/output/{{workflow.uid}}/{{pod.name}}/pkgs',\n",
    "    bq_project_id = 'lf-ml-demo',\n",
    "    bq_dataset = 'spark_demo',\n",
    "    bq_table = 'short_data',\n",
    "    output_path = 'gs://{0}/output/{{workflow.uid}}/{{pod.name}}/test.csv'.format(BUCKET_NAME),\n",
    "    args='',\n",
    "    job='{}',\n",
    "    wait_interval='30'\n",
    "    ):\n",
    "    \n",
    "    \n",
    "    delete_cluster_task = dataproc_delete_cluster_op(\n",
    "        cluster_project_id,\n",
    "        cluster_region,\n",
    "        cluster_name\n",
    "    ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    delete_cluster_task.set_display_name('delete cluster')\n",
    "    \n",
    "    with dsl.ExitHandler(exit_op=delete_cluster_task):\n",
    "        #create cluster\n",
    "        create_cluster_task = dataproc_create_cluster_op(\n",
    "            project_id=cluster_project_id,\n",
    "            region=cluster_region,\n",
    "            name=cluster_name).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "        create_cluster_task.set_display_name('create cluster')\n",
    "        \n",
    "        #upload file to GCS\n",
    "        upload_files_to_GCS_task = upload_files_to_GCS_op(\n",
    "            output_gcs_path=gcs_pkgs_path).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "        upload_files_to_GCS_task.set_display_name('upload files')\n",
    "        \n",
    "        #submit job to dataproc cluster\n",
    "        dataproc_submit_pyspark_job_task = dataproc_submit_pyspark_job_op(\n",
    "            project_id=cluster_project_id, \n",
    "            region=cluster_region, \n",
    "            cluster_name=create_cluster_task.outputs['cluster_name'], \n",
    "            main_python_file_uri=upload_files_to_GCS_task.outputs['transform_run_path'], \n",
    "            args=args, \n",
    "            pyspark_job={\n",
    "                'main_python_file_uri': upload_files_to_GCS_task.outputs['transform_run_path'],\n",
    "                'jar_file_uris': upload_files_to_GCS_task.outputs['jar_path'],\n",
    "                'args' : ['--tableProjectID', bq_project_id, \n",
    "                          '--dataset', bq_dataset, \n",
    "                          '--table', bq_table,\n",
    "                          '--output', output_path]\n",
    "            },  \n",
    "            job=job, \n",
    "            wait_interval=wait_interval).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        \n",
    "        dataproc_submit_pyspark_job_task.set_display_name('run pyspark job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile pipeline to check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(end_to_end_pyspark, end_to_end_pyspark.__name__ + '.pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the pipeline to Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `GOOGLE_APPLICATION_CREDENTIALS` for dealing with authorisation. The service account has role `IAP-secured Web App User`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/lfloretta/.secrets/lf-ml-demo-20819be29240.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KfpClient(\n",
    "    host='https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline',\n",
    "    client_id='49311432881-9u2qfhilqci5fdthfsh8t0njpuugkj18.apps.googleusercontent.com',\n",
    "    namespace='kubeflow_lfloretta'\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_pipeline(\n",
    "    pipeline_package_path=end_to_end_pyspark.__name__ + '.pipeline.zip', \n",
    "    pipeline_name='e2e_pyspark_run') #make the name unique with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline from the UI"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
