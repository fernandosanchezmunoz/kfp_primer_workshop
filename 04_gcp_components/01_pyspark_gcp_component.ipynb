{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component to run a PySpark Transformation\n",
    "Data preparation using PySpark on Cloud Dataproc. Download a BigQuery Table using the BigQuery Storage API connector, and write the output to csv files on Google Cloud Storage. \n",
    "\n",
    "The [submit_pyspark_job](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/submit_pyspark_job) component creates a PySpark job from the [Dataproc submit job REST API](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit).\n",
    "\n",
    "\n",
    "# Details on the submit_pyspark component arguments\n",
    "\n",
    "### pyspark_submit component arguments\n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| project_id | The ID of the Google Cloud Platform (GCP) project that the cluster belongs to. | No | GCPProjectID |  |  |\n",
    "| region | The Cloud Dataproc region to handle the request. | No | GCPRegion |  |  |\n",
    "| cluster_name | The name of the cluster to run the job. | No | String |  |  |\n",
    "| main_python_file_uri | The HCFS URI of the Python file to use as the driver. This must be a .py file. | No | GCSPath |  |  |\n",
    "| args | The arguments to pass to the driver. See below | Yes | List |  | None |\n",
    "| pyspark_job | The payload of a [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob). | Yes | Dict |  | None |\n",
    "| job | The payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs). | Yes | Dict |  | None |\n",
    "\n",
    "### driver program args \n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| tableProjectID | The ID of the Google Cloud Platform (GCP) projec that the table belong to | No | GCPProjectID | |\n",
    "| table | The name of the BigQuery table to download | No | String | |\n",
    "| dataset | The name of the BigQuery dataset to download the table from | No | String | |\n",
    "| output | The output file name and location. gs://bucket/output/file.csv | No | String | |\n",
    "\n",
    "\n",
    "### Output\n",
    "Name | Description | Type\n",
    ":--- | :---------- | :---\n",
    "job_id | The ID of the created job. | String\n",
    "\n",
    "\n",
    "# Setup & requirements for a test\n",
    "\n",
    "To run the pipeline, you must:\n",
    "*   Set up a GCP project by following this [guide](https://cloud.google.com/dataproc/docs/guides/setup-project).\n",
    "*   [Create a new cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster).\n",
    "*   Create a Google Cloud Storage bucket, to hold your dependencies and out. Follow this [guide](https://cloud.google.com/storage/docs/creating-buckets).\n",
    "*   Grant the Kubeflow user service account the role `roles/dataproc.editor` on the project.\n",
    "*   Grant the [default compute service account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts), used by DataProc, the role of `roles/bigquery.user`. This is the `[project-number]-compute@developer.gserviceaccount.com` service account.\n",
    "\n",
    "### Copy dependencies to Google Cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "BUCKET_NAME = 'lf-ml-demo-eu-w1/kfp_primer/test/01/dataproc'\n",
    "MAIN_FILE_PATH = 'gs://{}/transform_run.py'.format(BUCKET_NAME)\n",
    "JAR_PATH = 'gs://{}/sparkicson-0.1-dependencies.jar'.format(BUCKET_NAME)\n",
    "os.environ['MAIN_FILE_PATH'] = MAIN_FILE_PATH\n",
    "os.environ['JAR_PATH'] = JAR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the main python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./pyspark_job/src/transform_run.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  3.1 KiB]                                                \r",
      "/ [1 files][  3.1 KiB/  3.1 KiB]                                                \r\n",
      "Operation completed over 1 objects/3.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp ./pyspark_job/src/transform_run.py $MAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://./pyspark_job/src/target/sparkicson-0.1-dependencies.jar [Content-Type=application/java-archive]...\n",
      "/ [0 files][    0.0 B/ 32.8 MiB]                                                \r",
      "/ [0 files][264.0 KiB/ 32.8 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][  5.2 MiB/ 32.8 MiB]                                                \r",
      "|\r",
      "| [0 files][  7.5 MiB/ 32.8 MiB]                                                \r",
      "/\r",
      "/ [0 files][ 11.3 MiB/ 32.8 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 14.4 MiB/ 32.8 MiB]                                                \r",
      "|\r",
      "/\r",
      "/ [0 files][ 18.6 MiB/ 32.8 MiB]                                                \r",
      "-\r",
      "\\\r",
      "\\ [0 files][ 21.9 MiB/ 32.8 MiB]                                                \r",
      "|\r",
      "| [0 files][ 25.3 MiB/ 32.8 MiB]                                                \r",
      "/\r",
      "-\r",
      "- [0 files][ 28.9 MiB/ 32.8 MiB]                                                \r",
      "\\\r",
      "|\r",
      "| [0 files][ 32.2 MiB/ 32.8 MiB]                                                \r",
      "| [1 files][ 32.8 MiB/ 32.8 MiB]    2.0 MiB/s                                   \r",
      "/\r\n",
      "Operation completed over 1 objects/32.8 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp ./pyspark_job/src/target/sparkicson-0.1-dependencies.jar $JAR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataproc_submit_pyspark component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dataproc_submit_pyspark_job:\n",
      "\n",
      "dataproc_submit_pyspark_job(project_id: 'GCPProjectID', region: 'GCPRegion', cluster_name: 'String', main_python_file_uri: 'GCSPath', args: 'List' = '', pyspark_job: 'Dict' = '', job: 'Dict' = '', wait_interval: 'Integer' = '30')\n",
      "    dataproc_submit_pyspark_job\n",
      "    Submits a Cloud Dataproc job for running Apache PySpark applications on YARN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "dataproc_submit_pyspark_job_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/a97f1d0ad0e7b92203f35c5b0b9af3a314952e05/components/gcp/dataproc/submit_pyspark_job/component.yaml')\n",
    "help(dataproc_submit_pyspark_job_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Dataproc submit PySpark job pipeline',\n",
    "    description='Dataproc submit PySpark job pipeline'\n",
    ")\n",
    "def dataproc_submit_pyspark_job_pipeline(\n",
    "    cluster_project_id = 'lf-ml-demo', \n",
    "    cluster_region = 'europe-west1',\n",
    "    cluster_name = 'cluster-edc5',\n",
    "    bq_project_id = 'bigquery-samples',\n",
    "    bq_dataset = 'wikipedia_benchmark',\n",
    "    bq_table = 'Wiki10M',\n",
    "    output_path = 'gs://{0}/output/{{{{workflow.uid}}}}/{{{{pod.name}}}}/test.csv'.format(BUCKET_NAME),\n",
    "    main_python_file_uri = '{0}'.format(MAIN_FILE_PATH), \n",
    "    jar_file_uris = '{0}'.format(JAR_PATH),\n",
    "    args = '', \n",
    "    job='{}', \n",
    "    wait_interval='30'\n",
    "):\n",
    "    \n",
    "    dataproc_submit_pyspark_job_op(\n",
    "        project_id=cluster_project_id, \n",
    "        region=cluster_region, \n",
    "        cluster_name=cluster_name, \n",
    "        main_python_file_uri=main_python_file_uri, \n",
    "        args=args, \n",
    "        pyspark_job=json.dumps({\n",
    "            'main_python_file_uri': main_python_file_uri,\n",
    "            'jar_file_uris': jar_file_uris,\n",
    "            'args' : ['--tableProjectID', bq_project_id, \n",
    "                      '--dataset', bq_dataset, \n",
    "                      '--table', bq_table,\n",
    "                      '--output', output_path]\n",
    "        }), \n",
    "        job=job, \n",
    "        wait_interval=wait_interval).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type PipelineParam is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5e67ca3cdfac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpipeline_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, pipeline_func, package_path, type_check)\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0mkfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTYPE_CHECK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0mworkflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m       \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDumper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_aliases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m       \u001b[0myaml_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_flow_style\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/site-packages/kfp/compiler/compiler.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(self, pipeline_func)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mdsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m       \u001b[0mpipeline_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# Remove when argo supports local exit handler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-25473f52362b>\u001b[0m in \u001b[0;36mdataproc_submit_pyspark_job_pipeline\u001b[0;34m(cluster_project_id, cluster_region, cluster_name, bq_project_id, bq_dataset, bq_table, output_path, main_python_file_uri, jar_file_uris, args, job, wait_interval)\u001b[0m\n\u001b[1;32m     33\u001b[0m                       \u001b[0;34m'--dataset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                       \u001b[0;34m'--table'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                       '--output', output_path]\n\u001b[0m\u001b[1;32m     36\u001b[0m         }), \n\u001b[1;32m     37\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m~/DevTool/miniconda3/envs/kfp/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type PipelineParam is not JSON serializable"
     ]
    }
   ],
   "source": [
    "pipeline_func = dataproc_submit_pyspark_job_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.zip'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pipeline\n",
    "\n",
    "Set `GOOGLE_APPLICATION_CREDENTIALS` for dealing with authorisation. The service account has role `IAP-secured Web App User`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/lfloretta/.secrets/lf-ml-demo-20819be29240.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client as KfpClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KfpClient(\n",
    "    host='https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline',\n",
    "    client_id='49311432881-9u2qfhilqci5fdthfsh8t0njpuugkj18.apps.googleusercontent.com',\n",
    "    namespace='kubeflow_lfloretta'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_pipeline(\n",
    "    pipeline_package_path=pipeline_filename, \n",
    "    pipeline_name='pyspark_run_test_12') #make the name unique with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline from the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*   [Create a new Dataproc cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster) \n",
    "*   [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)\n",
    "*   [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
