{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component to run a PySpark Transformation\n",
    "Data preparation using PySpark on Cloud Dataproc. Download a BigQuery Table using the BigQuery Storage API connector, and write the output to csv files on Google Cloud Storage. \n",
    "\n",
    "The [submit_pyspark_job](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/submit_pyspark_job) component creates a PySpark job from the [Dataproc submit job REST API](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit).\n",
    "\n",
    "\n",
    "# Details on the submit_pyspark component arguments\n",
    "\n",
    "### pyspark_submit component arguments\n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| project_id | The ID of the Google Cloud Platform (GCP) project that the cluster belongs to. | No | GCPProjectID |  |  |\n",
    "| region | The Cloud Dataproc region to handle the request. | No | GCPRegion |  |  |\n",
    "| cluster_name | The name of the cluster to run the job. | No | String |  |  |\n",
    "| main_python_file_uri | The HCFS URI of the Python file to use as the driver. This must be a .py file. | No | GCSPath |  |  |\n",
    "| args | The arguments to pass to the driver. See below | Yes | List |  | None |\n",
    "| pyspark_job | The payload of a [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob). | Yes | Dict |  | None |\n",
    "| job | The payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs). | Yes | Dict |  | None |\n",
    "\n",
    "### driver program args \n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| tableProjectID | The ID of the Google Cloud Platform (GCP) projec that the table belong to | No | GCPProjectID | |\n",
    "| table | The name of the BigQuery table to download | No | String | |\n",
    "| dataset | The name of the BigQuery dataset to download the table from | No | String | |\n",
    "| output | The output file name and location. gs://bucket/output/file.csv | No | String | |\n",
    "\n",
    "\n",
    "### Output\n",
    "Name | Description | Type\n",
    ":--- | :---------- | :---\n",
    "job_id | The ID of the created job. | String\n",
    "\n",
    "\n",
    "# Setup & requirements for a test\n",
    "\n",
    "To run the pipeline, you must:\n",
    "*   Set up a GCP project by following this [guide](https://cloud.google.com/dataproc/docs/guides/setup-project).\n",
    "*   [Create a new cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster).\n",
    "*   Create a Google Cloud Storage bucket, to hold your dependencies and out. Follow this [guide](https://cloud.google.com/storage/docs/creating-buckets).\n",
    "*   Grant the Kubeflow user service account the role `roles/dataproc.editor` on the project.\n",
    "*   Grant the [default compute service account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts), used by DataProc, the role of `roles/bigquery.user`. This is the `[project-number]-compute@developer.gserviceaccount.com` service account.\n",
    "\n",
    "### Copy dependencies to Google Cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "BUCKET_NAME = 'lf-ml-demo-eu-w1/test_test' ### input your bucket\n",
    "MAIN_FILE_PATH = 'gs://{}/transform_run.py'.format(BUCKET_NAME)\n",
    "JAR_PATH = 'gs://{}/sparkicson-0.1-dependencies.jar'.format(BUCKET_NAME)\n",
    "os.environ['MAIN_FILE_PATH'] = MAIN_FILE_PATH\n",
    "os.environ['JAR_PATH'] = JAR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would need to run the script `upload.sh` to upload the data to GCP in the cloud console. Make sure to pass the same value of `BUCKET_NAME`. \n",
    "\n",
    "The reason why we need to follow this appraoch is that the node pool does not have the scope to write on GCS.\n",
    "![scope](img/original_scope.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataproc_submit_pyspark component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "dataproc_submit_pyspark_job_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/a97f1d0ad0e7b92203f35c5b0b9af3a314952e05/components/gcp/dataproc/submit_pyspark_job/component.yaml')\n",
    "help(dataproc_submit_pyspark_job_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Dataproc submit PySpark job pipeline',\n",
    "    description='Dataproc submit PySpark job pipeline'\n",
    ")\n",
    "def dataproc_submit_pyspark_job_pipeline(\n",
    "    cluster_project_id = 'kfp-primer-workshop', \n",
    "    cluster_region = 'us-central1', \n",
    "    cluster_name = 'cluster-5d99',\n",
    "    bq_project_id = 'bigquery-samples',\n",
    "    bq_dataset = 'wikipedia_benchmark',\n",
    "    bq_table = 'Wiki10M',\n",
    "    output_path = 'gs://{0}/output/{{{{workflow.uid}}}}/{{{{pod.name}}}}/test.csv'.format(BUCKET_NAME),\n",
    "    main_python_file_uri = '{0}'.format(MAIN_FILE_PATH), \n",
    "    jar_file_uris = '{0}'.format(JAR_PATH),\n",
    "    args = '', \n",
    "    job='{}', \n",
    "    wait_interval='30'\n",
    "):\n",
    "    \n",
    "    dataproc_submit_pyspark_job_op(\n",
    "        project_id=cluster_project_id, \n",
    "        region=cluster_region, \n",
    "        cluster_name=cluster_name, \n",
    "        main_python_file_uri=main_python_file_uri, \n",
    "        args=args, \n",
    "        pyspark_job=json.dumps({\n",
    "            'main_python_file_uri': str(main_python_file_uri),\n",
    "            'jar_file_uris': str(jar_file_uris),\n",
    "            'args' : ['--tableProjectID', str(bq_project_id), \n",
    "                      '--dataset', str(bq_dataset), \n",
    "                      '--table', str(bq_table),\n",
    "                      '--output', str(output_path)]\n",
    "        }), \n",
    "        job=job, \n",
    "        wait_interval=wait_interval).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = dataproc_submit_pyspark_job_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.zip'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pipeline\n",
    "\n",
    "If running outside of the cluster with Kubeflow, set `GOOGLE_APPLICATION_CREDENTIALS` for dealing with authorisation. The service account needs to have the role `IAP-secured Web App User`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '' # path to the json file of the service account used to log in: it need to have role IAP-secured Web App User\n",
    "# HOST = '' # url of the cluster e.g. https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline\n",
    "# CLIENT_ID = '' # The client ID used by Identity-Aware Proxy\n",
    "# NAMESPACE = '' # user namespace e.g. https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client as KfpClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KfpClient(\n",
    "# we are running into the same Kubeflow so we do not need to do anything\n",
    "#     host=HOST,\n",
    "#     client_id=CLIENT_ID,\n",
    "#     namespace=NAMESPACE  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_pipeline(\n",
    "    pipeline_package_path=pipeline_filename, \n",
    "    pipeline_name='pyspark_run_test_001') #make the name unique with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline from the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*   [Create a new Dataproc cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster) \n",
    "*   [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)\n",
    "*   [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
