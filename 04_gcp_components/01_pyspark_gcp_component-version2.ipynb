{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component to run a PySpark Transformation\n",
    "Data preparation using PySpark on Cloud Dataproc. Download a BigQuery Table using the BigQuery Storage API connector, and write the output to csv files on Google Cloud Storage. \n",
    "\n",
    "The [submit_pyspark_job](https://github.com/kubeflow/pipelines/tree/master/components/gcp/dataproc/submit_pyspark_job) component creates a PySpark job from the [Dataproc submit job REST API](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit).\n",
    "\n",
    "\n",
    "# Details on the submit_pyspark component arguments\n",
    "\n",
    "### pyspark_submit component arguments\n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| project_id | The ID of the Google Cloud Platform (GCP) project that the cluster belongs to. | No | GCPProjectID |  |  |\n",
    "| region | The Cloud Dataproc region to handle the request. | No | GCPRegion |  |  |\n",
    "| cluster_name | The name of the cluster to run the job. | No | String |  |  |\n",
    "| main_python_file_uri | The HCFS URI of the Python file to use as the driver. This must be a .py file. | No | GCSPath |  |  |\n",
    "| args | The arguments to pass to the driver. See below | Yes | List |  | None |\n",
    "| pyspark_job | The payload of a [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob). | Yes | Dict |  | None |\n",
    "| job | The payload of a [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs). | Yes | Dict |  | None |\n",
    "\n",
    "### driver program args \n",
    "| Argument | Description | Optional | Data type | Accepted values | Default |\n",
    "|----------------------|------------|----------|--------------|-----------------|---------|\n",
    "| tableProjectID | The ID of the Google Cloud Platform (GCP) projec that the table belong to | No | GCPProjectID | |\n",
    "| table | The name of the BigQuery table to download | No | String | |\n",
    "| dataset | The name of the BigQuery dataset to download the table from | No | String | |\n",
    "| output | The output file name and location. gs://bucket/output/file.csv | No | String | |\n",
    "\n",
    "\n",
    "### Output\n",
    "Name | Description | Type\n",
    ":--- | :---------- | :---\n",
    "job_id | The ID of the created job. | String\n",
    "\n",
    "\n",
    "# Setup & requirements for a test\n",
    "\n",
    "To run the pipeline, you must:\n",
    "*   Set up a GCP project by following this [guide](https://cloud.google.com/dataproc/docs/guides/setup-project).\n",
    "*   [Create a new cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster).\n",
    "*   Create a Google Cloud Storage bucket, to hold your dependencies and out. Follow this [guide](https://cloud.google.com/storage/docs/creating-buckets).\n",
    "*   Grant the Kubeflow user service account the role `roles/dataproc.editor` on the project.\n",
    "*   Grant the [default compute service account](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts), used by DataProc, the role of `roles/bigquery.user`. This is the `[project-number]-compute@developer.gserviceaccount.com` service account.\n",
    "\n",
    "### Copy dependencies to Google Cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "BUCKET_NAME = 'lf-ml-demo-eu-w1/kfp_primer/test/01/dataproc'\n",
    "MAIN_FILE_PATH = 'gs://{}/transform_run.py'.format(BUCKET_NAME)\n",
    "JAR_PATH = 'gs://{}/sparkicson-0.1-dependencies.jar'.format(BUCKET_NAME)\n",
    "os.environ['MAIN_FILE_PATH'] = MAIN_FILE_PATH\n",
    "os.environ['JAR_PATH'] = JAR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the main python file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp ./pyspark_job/src/transform_run.py $MAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp ./pyspark_job/src/target/sparkicson-0.1-dependencies.jar $JAR_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataproc_submit_pyspark component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dataproc_submit_pyspark_job:\n",
      "\n",
      "dataproc_submit_pyspark_job(project_id: 'GCPProjectID', region: 'GCPRegion', cluster_name: 'String', main_python_file_uri: 'GCSPath', args: 'List' = '', pyspark_job: 'Dict' = '', job: 'Dict' = '', wait_interval: 'Integer' = '30')\n",
      "    dataproc_submit_pyspark_job\n",
      "    Submits a Cloud Dataproc job for running Apache PySpark applications on YARN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "dataproc_submit_pyspark_job_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/a97f1d0ad0e7b92203f35c5b0b9af3a314952e05/components/gcp/dataproc/submit_pyspark_job/component.yaml')\n",
    "help(dataproc_submit_pyspark_job_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dict_arguments(my_dict: dict) -> None:\n",
    "    print(type(my_dict))\n",
    "    print(dict))\n",
    "    \n",
    "    if 'args' in my_dict:\n",
    "        print(my_dict['args'])\n",
    "    else:\n",
    "        print(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict_arguments_op = comp.func_to_container_op(func=test_dict_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pyspark_job_dict(\n",
    "    main_python_file_uri: str, jar_file_uris: str, bq_project_id: str, \n",
    "    bq_dataset: str, bq_table: str, output_path: str) -> dict:\n",
    "    \n",
    "    pyspark_job = {'main_python_file_uri': main_python_file_uri,\n",
    "            'jar_file_uris': jar_file_uris}\n",
    "    \n",
    "    print(pyspark_job)\n",
    "    \n",
    "    return pyspark_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_python_file_uri': 'main_python_file_uri', 'jar_file_uris': 'jar_file_uris'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'main_python_file_uri': 'main_python_file_uri',\n",
       " 'jar_file_uris': 'jar_file_uris'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_pyspark_job_dict(main_python_file_uri='main_python_file_uri',\n",
    "        jar_file_uris='jar_file_uris',\n",
    "        bq_project_id='bq_project_id', \n",
    "        bq_dataset='bq_dataset',\n",
    "        bq_table='bq_table',\n",
    "        output_path='output_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_pyspark_job_dict_op = comp.func_to_container_op(create_pyspark_job_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Dataproc submit PySpark job pipeline',\n",
    "    description='Dataproc submit PySpark job pipeline'\n",
    ")\n",
    "def dataproc_submit_pyspark_job_pipeline(\n",
    "    cluster_project_id = 'lf-ml-demo', \n",
    "    cluster_region = 'europe-west1',\n",
    "    cluster_name = 'cluster-edc5',\n",
    "    bq_project_id = 'bigquery-samples',\n",
    "    bq_dataset = 'wikipedia_benchmark',\n",
    "    bq_table = 'Wiki10M',\n",
    "    #output_path = 'gs://{0}/output/{{workflow.uid}}/{{pod.name}}/test.csv'.format(BUCKET_NAME),\n",
    "    output_path = 'gs://{0}/output/test.csv'.format(BUCKET_NAME),\n",
    "    main_python_file_uri = '{0}'.format(MAIN_FILE_PATH), \n",
    "    jar_file_uris = '{0}'.format(JAR_PATH),\n",
    "    args = '', \n",
    "    job='{}', \n",
    "    wait_interval='30'\n",
    "):\n",
    "    create_pyspark_job_dict_task = create_pyspark_job_dict_op(\n",
    "        main_python_file_uri=main_python_file_uri,\n",
    "        jar_file_uris=jar_file_uris,\n",
    "        bq_project_id=bq_project_id, \n",
    "        bq_dataset=bq_dataset,\n",
    "        bq_table=bq_table,\n",
    "        output_path=output_path)\n",
    "    \n",
    "    test_dict_arguments_task = test_dict_arguments_op(my_dict=create_pyspark_job_dict_task.output)\n",
    "\n",
    "#     dataproc_submit_pyspark_job_op(\n",
    "#         project_id=cluster_project_id, \n",
    "#         region=cluster_region, \n",
    "#         cluster_name=cluster_name, \n",
    "#         main_python_file_uri=main_python_file_uri, \n",
    "#         args=args, \n",
    "#         pyspark_job=pyspark_job, \n",
    "#         job=job, \n",
    "#         wait_interval=wait_interval).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = dataproc_submit_pyspark_job_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.zip'\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the pipeline\n",
    "\n",
    "Set `GOOGLE_APPLICATION_CREDENTIALS` for dealing with authorisation. The service account has role `IAP-secured Web App User`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/lfloretta/.secrets/lf-ml-demo-20819be29240.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client as KfpClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = KfpClient(\n",
    "    host='https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline',\n",
    "    client_id='49311432881-9u2qfhilqci5fdthfsh8t0njpuugkj18.apps.googleusercontent.com',\n",
    "    namespace='kubeflow_lfloretta'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Pipeline link <a href=https://demo-kubeflow.endpoints.lf-ml-demo.cloud.goog/pipeline/#/pipelines/details/da4e6d42-bd89-48f4-956d-c04f3ec2d29a>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2019, 9, 10, 9, 30, 29, tzinfo=tzutc()),\n",
       " 'description': None,\n",
       " 'error': None,\n",
       " 'id': 'da4e6d42-bd89-48f4-956d-c04f3ec2d29a',\n",
       " 'name': 'pyspark_run_test_10',\n",
       " 'parameters': [{'name': 'cluster-project-id', 'value': 'lf-ml-demo'},\n",
       "                {'name': 'cluster-region', 'value': 'europe-west1'},\n",
       "                {'name': 'cluster-name', 'value': 'cluster-edc5'},\n",
       "                {'name': 'bq-project-id', 'value': 'bigquery-samples'},\n",
       "                {'name': 'bq-dataset', 'value': 'wikipedia_benchmark'},\n",
       "                {'name': 'bq-table', 'value': 'Wiki10M'},\n",
       "                {'name': 'output-path',\n",
       "                 'value': 'gs://lf-ml-demo-eu-w1/kfp_primer/test/01/dataproc/output/test.csv'},\n",
       "                {'name': 'main-python-file-uri',\n",
       "                 'value': 'gs://lf-ml-demo-eu-w1/kfp_primer/test/01/dataproc/transform_run.py'},\n",
       "                {'name': 'jar-file-uris',\n",
       "                 'value': 'gs://lf-ml-demo-eu-w1/kfp_primer/test/01/dataproc/sparkicson-0.1-dependencies.jar'},\n",
       "                {'name': 'args', 'value': None},\n",
       "                {'name': 'job', 'value': '{}'},\n",
       "                {'name': 'wait-interval', 'value': '30'}],\n",
       " 'url': None}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upload_pipeline(\n",
    "    pipeline_package_path=pipeline_filename, \n",
    "    pipeline_name='pyspark_run_test_10') #make the name unique with your username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline from the UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*   [Create a new Dataproc cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster) \n",
    "*   [PySparkJob](https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob)\n",
    "*   [Dataproc job](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
